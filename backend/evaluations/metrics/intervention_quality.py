"""
LLM-as-judge metric: Is the intervention appropriate, actionable while driving,
and personalized to user preferences?
"""
import os
import json
from typing import Any
from opik.evaluation.metrics import BaseMetric
from opik.evaluation.metrics.score_result import ScoreResult


QUALITY_JUDGE_PROMPT = """You are evaluating a calming intervention generated by Serene, a driving anxiety support AI.

**Context:**
- Stress level: {stress_level} (score: {stress_score})
- Intervention type: {intervention_type}
- User's calming preferences: {user_preferences}
- Driving situation: {context}

**AI's response:**
---
{response}
---

Rate the QUALITY of this intervention on a 0.0 to 1.0 scale. Consider:

1. **Appropriateness** (0.3 weight): Is the intervention type right for this stress level?
2. **Personalization** (0.25 weight): Does it align with the user's calming preferences?
3. **Actionability** (0.25 weight): Can the driver follow this while driving safely?
4. **Brevity** (0.2 weight): Is it concise enough for a stressed driver? (Not too wordy)

Respond with ONLY a JSON object:
{{"score": <float 0.0-1.0>, "reason": "<brief explanation>"}}
"""


class InterventionQualityMetric(BaseMetric):
    """Evaluate overall quality of calming interventions."""

    name = "intervention_quality"

    def __init__(self, model: str = "gpt-4o-mini"):
        super().__init__(name=self.name)
        self._model = model

    def score(
        self,
        output: str,
        stress_level: str = "MEDIUM",
        stress_score: float = 0.5,
        intervention_type: str = "CALMING_MESSAGE",
        user_preferences: str = "",
        context: str = "",
        **ignored_kwargs: Any,
    ) -> ScoreResult:
        try:
            from openai import OpenAI

            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            prompt = QUALITY_JUDGE_PROMPT.format(
                stress_level=stress_level,
                stress_score=stress_score,
                intervention_type=intervention_type,
                user_preferences=user_preferences or "Not specified",
                context=context or "General driving",
                response=output,
            )
            response = client.chat.completions.create(
                model=self._model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=200,
            )
            content = response.choices[0].message.content.strip()
            parsed = json.loads(content)
            return ScoreResult(
                value=max(0.0, min(1.0, float(parsed["score"]))),
                name=self.name,
                reason=parsed.get("reason", "LLM quality evaluation"),
            )
        except Exception as e:
            return ScoreResult(
                value=0.5,
                name=self.name,
                reason=f"LLM judge failed: {e}",
            )
